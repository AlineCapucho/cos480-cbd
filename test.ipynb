{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel('./dataset/Coffee Shop Sales.xlsx')\n",
    "data.to_csv('./dataset/Coffee Shop Sales.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = pd.read_csv('./dataset/Coffee Shop Sales.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'transaction_id': 6, 'transaction_date': 10, 'transaction_time': 8, 'transaction_qty': 1, 'store_id': 1, 'store_location': 15, 'product_id': 2, 'unit_price': 5, 'product_category': 18, 'product_type': 21, 'product_detail': 28}\n",
      "[6, 10, 8, 1, 1, 15, 2, 5, 18, 21, 28]\n"
     ]
    }
   ],
   "source": [
    "columns = data2.columns\n",
    "size_per_column = dict()\n",
    "for column in columns:\n",
    "    elements = data2[column].unique()\n",
    "    elements_size = [len(str(elem)) for elem in elements]\n",
    "    size_per_column[column] = max(elements_size)\n",
    "print(size_per_column)\n",
    "print(list(size_per_column.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fixed_Size_Heap:\n",
    "    def __init__(self, block_size, field_sizes, filename):\n",
    "        self.block_size = block_size\n",
    "        self.field_sizes = field_sizes\n",
    "        self._set_record_size()\n",
    "        self.blocks = []\n",
    "        self._read_file(filename)\n",
    "        self.deleted_records = []\n",
    "\n",
    "    def _set_field_names(self, line):\n",
    "        self.field_names = line.split(',')\n",
    "\n",
    "    def _set_record_size(self):\n",
    "        self.record_size = sum(self.field_sizes) + len(self.field_sizes)\n",
    "    \n",
    "    def _padding(self, field, field_id):\n",
    "        diff = self.field_sizes[field_id] - len(field)\n",
    "        padded_field = field + (' ' * diff)\n",
    "        return padded_field\n",
    "\n",
    "    def _format_record(self, record):\n",
    "        formatted_record = ''\n",
    "        fields = record.strip().split(',')\n",
    "        for i in range(len(fields)):\n",
    "            if len(fields[i]) < self.field_sizes[i]:\n",
    "                padded_field = self._padding(fields[i], i)\n",
    "                formatted_record += padded_field + ','\n",
    "            else:\n",
    "                formatted_record += fields[i] + ','\n",
    "        return formatted_record\n",
    "\n",
    "    def _write_record(self, record):\n",
    "        if self.blocks == []:\n",
    "            self.blocks.append(self._format_record(record))\n",
    "        elif len(self.blocks[-1]) + self.record_size < self.block_size:\n",
    "            self.blocks[-1] += self._format_record(record)\n",
    "        else:\n",
    "            self.blocks.append(self._format_record(record))\n",
    "\n",
    "    def _read_file(self, filename):\n",
    "        with open(filename, 'r') as file:\n",
    "            self._set_field_names(file.readline())\n",
    "            for record in file:\n",
    "                self._write_record(record)\n",
    "    \n",
    "    def _delete_record(self, block_id, record_id):\n",
    "        offset = self.record_size * record_id\n",
    "        head = self.blocks[block_id][:offset]\n",
    "        body = ' ' * self.record_size\n",
    "        tail = self.blocks[block_id][offset + self.record_size:]\n",
    "        self.blocks[block_id] = head + body + tail\n",
    "        self.deleted_records.append([block_id, record_id])\n",
    "\n",
    "    def delete_record_by_primary_key(self, key):\n",
    "        field_key_size = self.field_sizes[0]\n",
    "        success = False\n",
    "        for i in range(len(self.blocks)):\n",
    "            if success:\n",
    "                break\n",
    "            number_of_records = math.floor(self.block_size / self.record_size)\n",
    "            for j in range(0, number_of_records):\n",
    "                offset = self.record_size * j\n",
    "                print('offset: ', offset)\n",
    "                print('field_key_size: ', field_key_size)\n",
    "                print('block section: ', self.blocks[i][offset:field_key_size])\n",
    "                block_key = int(self.blocks[i][offset:field_key_size].strip())\n",
    "                if block_key == key:\n",
    "                    self._delete_record(i, j)\n",
    "                    success = True\n",
    "                    break\n",
    "        if not success:\n",
    "            raise Exception('DeleteError: Primary Key nonexistent.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = './dataset/Coffee Shop Sales.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "myfile = Fixed_Size_Heap(\n",
    "                    block_size=512,\n",
    "                    field_sizes=list(size_per_column.values()),\n",
    "                    filename=filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1     ,2023-01-01,07:06:11,2,5,Lower Manhattan,32,3.0  ,Coffee            ,Gourmet brewed coffee,Ethiopia Rg                 ,2     ,2023-01-01,07:08:56,2,5,Lower Manhattan,57,3.1  ,Tea               ,Brewed Chai tea      ,Spicy Eye Opener Chai Lg    ,3     ,2023-01-01,07:14:04,2,5,Lower Manhattan,59,4.5  ,Drinking Chocolate,Hot chocolate        ,Dark chocolate Lg           ,4     ,2023-01-01,07:20:24,1,5,Lower Manhattan,22,2.0  ,Coffee            ,Drip coffee          ,Our Old Time Diner Blend Sm ,'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myfile.blocks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "offset:  0\n",
      "field_key_size:  6\n",
      "block section:  1     \n"
     ]
    }
   ],
   "source": [
    "myfile.delete_record_by_primary_key(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'                                                                                                                              2     ,2023-01-01,07:08:56,2,5,Lower Manhattan,57,3.1  ,Tea               ,Brewed Chai tea      ,Spicy Eye Opener Chai Lg    ,3     ,2023-01-01,07:14:04,2,5,Lower Manhattan,59,4.5  ,Drinking Chocolate,Hot chocolate        ,Dark chocolate Lg           ,4     ,2023-01-01,07:20:24,1,5,Lower Manhattan,22,2.0  ,Coffee            ,Drip coffee          ,Our Old Time Diner Blend Sm ,'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myfile.blocks[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "General_Work_Kernel",
   "language": "python",
   "name": "general_work_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
