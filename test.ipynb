{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "import math\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel('./dataset/Coffee Shop Sales.xlsx')\n",
    "data.to_csv('./dataset/Coffee Shop Sales.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = pd.read_csv('./dataset/Coffee Shop Sales.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = data2.columns\n",
    "size_per_column = dict()\n",
    "for column in columns:\n",
    "    elements = data2[column].unique()\n",
    "    elements_size = [len(str(elem)) for elem in elements]\n",
    "    size_per_column[column] = max(elements_size)\n",
    "print(size_per_column)\n",
    "print(list(size_per_column.values()))\n",
    "print(sum(list(size_per_column.values()))+len(list(size_per_column)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_type(value):\n",
    "    # Check for integer\n",
    "    if re.match(r'^[+-]?\\d+$', value):\n",
    "        return 'int'\n",
    "    \n",
    "    # Check for float\n",
    "    elif re.match(r'^[+-]?\\d*\\.\\d+$', value):\n",
    "        return 'float'\n",
    "    \n",
    "    # Check for date (format: YYYY-MM-DD)\n",
    "    elif re.match(r'^\\d{4}-\\d{2}-\\d{2}$', value):\n",
    "        try:\n",
    "            datetime.strptime(value, '%Y-%m-%d')\n",
    "            return 'date'\n",
    "        except ValueError:\n",
    "            return 'string'  # Caso não seja uma data válida\n",
    "    \n",
    "    # Check for time (format: HH:MM:SS)\n",
    "    elif re.match(r'^\\d{2}:\\d{2}:\\d{2}$', value):\n",
    "        try:\n",
    "            datetime.strptime(value, '%H:%M:%S')\n",
    "            return 'time'\n",
    "        except ValueError:\n",
    "            return 'string'  # Caso não seja um horário válido\n",
    "    \n",
    "    # If none of the above, treat it as a string\n",
    "    else:\n",
    "        return 'string'\n",
    "\n",
    "def infer_types_from_record(record, record_length):\n",
    "    fields = record.strip().split(',')\n",
    "    return [infer_type(field.strip()) for field in fields][:record_length]\n",
    "\n",
    "def range_between_integers(start, end):\n",
    "    ints = [i for i in range(int(start), int(end))]\n",
    "    return ints\n",
    "\n",
    "def range_between_dates(start, end):\n",
    "    dates_in_datetime = pd.date_range(start=start,end=end).to_pydatetime().tolist()\n",
    "    dates = [datetime.strftime(elem, '%Y-%m-%d') for elem in dates_in_datetime]\n",
    "    return dates\n",
    "\n",
    "def range_between_times(start, end):\n",
    "    times_in_datetime = pd.date_range(start, end, freq=\"1s\")\n",
    "    times = [datetime.strftime(elem, '%H:%M:%S') for elem in times_in_datetime]\n",
    "    return times\n",
    "\n",
    "def generate_range(range_type, start, end):\n",
    "    if range_type == 'int':\n",
    "        return range_between_integers(start, end)\n",
    "    elif range_type == 'date':\n",
    "        return range_between_dates(start, end)\n",
    "    elif range_type == 'time':\n",
    "        return range_between_times(start, end)\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "def check_interval(interval_type, start, end):\n",
    "    if interval_type == 'int':\n",
    "        if start == end or start > end:\n",
    "            return -1\n",
    "        return 0\n",
    "    elif interval_type == 'date':\n",
    "        start_date = datetime.strptime(start, '%Y-%m-%d')\n",
    "        end_date = datetime.strptime(end, '%Y-%m-%d')\n",
    "\n",
    "        if start_date == end_date:\n",
    "            return -1\n",
    "        elif start_date > end_date:\n",
    "            return -1\n",
    "        else:\n",
    "            return 0\n",
    "    elif interval_type == 'time':\n",
    "        start_time = datetime.strptime(start, '%H:%M:%S')\n",
    "        end_time = datetime.strptime(end, '%H:%M:%S')\n",
    "\n",
    "        if start_time == end_time:\n",
    "            return -1\n",
    "        elif start_time > end_time:\n",
    "            return -1\n",
    "        else:\n",
    "            return 0\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fixed_Size_Heap:\n",
    "    def __init__(self, block_size, field_sizes, filename):\n",
    "        self.block_size = block_size\n",
    "        self.field_sizes = field_sizes\n",
    "        self._set_record_size()\n",
    "        self.blocks = []\n",
    "        self._read_file(filename)\n",
    "        self.deleted_records = []\n",
    "\n",
    "    def _set_field_names(self, line):\n",
    "        self.field_names = line.split(',')\n",
    "\n",
    "    def _set_record_size(self):\n",
    "        self.record_size = sum(self.field_sizes) + len(self.field_sizes)\n",
    "    \n",
    "    def _set_field_types(self):\n",
    "        record = self.blocks[0][:self.record_size]\n",
    "        self.field_types = infer_types_from_record(record, len(self.field_names))\n",
    "    \n",
    "    def _padding(self, field, field_id):\n",
    "        diff = self.field_sizes[field_id] - len(field)\n",
    "        padded_field = field + (' ' * diff)\n",
    "        return padded_field\n",
    "\n",
    "    def _format_record(self, record):\n",
    "        formatted_record = ''\n",
    "        fields = record.strip().split(',')\n",
    "        for i in range(len(fields)):\n",
    "            if len(fields[i]) < self.field_sizes[i]:\n",
    "                padded_field = self._padding(fields[i], i)\n",
    "                formatted_record += padded_field + ','\n",
    "            else:\n",
    "                formatted_record += fields[i] + ','\n",
    "        return formatted_record\n",
    "\n",
    "    def _write_record(self, record):\n",
    "        if self.blocks == []:\n",
    "            self.blocks.append(self._format_record(record))\n",
    "        elif len(self.blocks[-1]) + self.record_size < self.block_size:\n",
    "            self.blocks[-1] += self._format_record(record)\n",
    "        else:\n",
    "            self.blocks.append(self._format_record(record))\n",
    "\n",
    "    def _read_file(self, filename):\n",
    "        with open(filename, 'r') as file:\n",
    "            self._set_field_names(file.readline())\n",
    "            for record in file:\n",
    "                self._write_record(record)\n",
    "            self._set_field_types()\n",
    "    \n",
    "    def _search(self, field_id, value):\n",
    "        field_size = self.field_sizes[field_id]\n",
    "        number_of_records = math.floor(self.block_size / self.record_size)\n",
    "        success = False\n",
    "        for i in range(len(self.blocks)):\n",
    "            if success and field_id == 0:\n",
    "                break\n",
    "            for j in range(0, number_of_records):\n",
    "                offset = self.record_size * j\n",
    "                if self.field_sizes[:field_id] != []:\n",
    "                    offset += sum(self.field_sizes[:field_id]) + field_id\n",
    "                field_value = self.blocks[i][offset:offset + field_size].strip()\n",
    "                if field_value == '':\n",
    "                    continue\n",
    "                if field_value == value:\n",
    "                    yield [i, j]\n",
    "                    success = True\n",
    "                    if field_id == 0:\n",
    "                        break\n",
    "        return [-1, -1]\n",
    "\n",
    "    def _select(self, select_container, block_id, record_id):\n",
    "        offset = self.record_size * record_id\n",
    "        record = self.blocks[block_id][offset:offset + self.record_size]\n",
    "        select_container.append(record)\n",
    "\n",
    "    def select_by_single_primary_key(self, key):\n",
    "        select_container = []\n",
    "        for (i, j) in self._search(field_id=0, value=key):\n",
    "            if i == -1 and j == -1:\n",
    "                raise Exception('SelectionError: Primary Key nonexistent.')\n",
    "            else:\n",
    "                self._select(select_container=select_container, block_id=i, record_id=j)\n",
    "        return select_container\n",
    "    \n",
    "    def select_by_multiple_primary_key(self, keys):\n",
    "        select_container = []\n",
    "        exception_counter = 0\n",
    "        for key in keys:\n",
    "            for (i, j) in self._search(field_id=0, value=key):\n",
    "                if i == -1 and j == -1:\n",
    "                    exception_counter += 1\n",
    "                else:\n",
    "                    self._select(select_container=select_container, block_id=i, record_id=j)\n",
    "        if exception_counter == len(keys):\n",
    "            raise Exception('SelectionError: Primary Keys nonexistent.')\n",
    "        return select_container\n",
    "    \n",
    "    def select_by_field_interval(self, field, start, end):\n",
    "        field_id = self.field_names.index(field)\n",
    "        field_type = self.field_types[field_id]\n",
    "        possible_field_interval = check_interval(interval_type=field_type, start=start, end=end)\n",
    "        if possible_field_interval == -1:\n",
    "            raise Exception('SelectionError: Field Interval incomputable.')\n",
    "        value_range = generate_range(range_type=field_type, start=start, end=end)\n",
    "        if value_range == -1:\n",
    "            raise Exception('SelectionError: Field Interval incomputable.')\n",
    "        select_container = []\n",
    "        exception_counter = 0\n",
    "        for value in value_range:\n",
    "            for (i, j) in self._search(field_id=field_id, value=str(value)):\n",
    "                if i == -1 and j == -1:\n",
    "                    exception_counter += 1\n",
    "                else:\n",
    "                    self._select(select_container=select_container, block_id=i, record_id=j)\n",
    "        if exception_counter == len(value_range):\n",
    "            raise Exception('SelectionError: Requested Records nonexistent.')\n",
    "        return select_container\n",
    "\n",
    "    def _delete_record(self, block_id, record_id):\n",
    "        offset = self.record_size * record_id\n",
    "        head = self.blocks[block_id][:offset]\n",
    "        body = ' ' * self.record_size\n",
    "        tail = self.blocks[block_id][offset + self.record_size:]\n",
    "        self.blocks[block_id] = head + body + tail\n",
    "        self.deleted_records.append([block_id, record_id])\n",
    "\n",
    "    def delete_record_by_primary_key(self, key):\n",
    "        for (i, j) in self._search(field_id=0, value=key):\n",
    "            if i == -1 and j == -1:\n",
    "                raise Exception('DeleteError: Primary Key nonexistent.')\n",
    "            else:\n",
    "                self._delete_record(block_id=i, record_id=j)\n",
    "    \n",
    "    def delete_record_by_criterion(self, field, value):\n",
    "        field_id = self.field_names.index(field)\n",
    "        for (i, j) in self._search(field_id=field_id, value=value):\n",
    "            if i == -1 and j == -1:\n",
    "                raise Exception('DeleteError: Field Value nonexistent.')\n",
    "            else:\n",
    "                self._delete_record(block_id=i, record_id=j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = './dataset/Coffee Shop Sales.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myfile = Fixed_Size_Heap(\n",
    "                    block_size=512,\n",
    "                    field_sizes=list(size_per_column.values()),\n",
    "                    filename=filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myfile.blocks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myfile.delete_record_by_primary_key('3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myfile.blocks[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myfile.delete_record_by_criterion(field='product_category', value='Coffee')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myfile.delete_record_by_criterion(field='transaction_date', value='2023-01-01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_container = myfile.select_by_single_primary_key('3')\n",
    "print(select_container)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_container = myfile.select_by_multiple_primary_key(['3', '5', '6'])\n",
    "print(select_container)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_container = myfile.select_by_field_interval(field='store_id', start='5', end='8')\n",
    "print(select_container[:5])\n",
    "print(len(select_container))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_container = myfile.select_by_field_interval(field='transaction_date', start='2023-01-01', end='2023-01-02')\n",
    "print(select_container[:5])\n",
    "print(len(select_container))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_container = myfile.select_by_field_interval(field='transaction_time', start='07:06:11', end='07:09:11')\n",
    "print(select_container[:5])\n",
    "print(len(select_container))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test.txt', 'r+') as file:\n",
    "    file.seek(10)\n",
    "    file.write('Testing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = ['ada', 'aba', 'aca']\n",
    "\n",
    "s = ','.join(str(elem) for elem in l)\n",
    "\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import shutil\n",
    "\n",
    "def comprimir_arquivo(nome_arquivo):\n",
    "    with open(nome_arquivo, 'rb') as f_in:\n",
    "        with gzip.open(nome_arquivo + '.gz', 'wb') as f_out:\n",
    "            shutil.copyfileobj(f_in, f_out)\n",
    "    print(f'Arquivo {nome_arquivo} comprimido para {nome_arquivo}.gz')\n",
    "\n",
    "# Exemplo de uso\n",
    "# comprimir_arquivo('./dataset/Coffee Shop Sales.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def descomprimir_arquivo(nome_arquivo):\n",
    "    with gzip.open(nome_arquivo, 'rb') as f_in:\n",
    "        with open(nome_arquivo[:-3], 'wb') as f_out:\n",
    "            shutil.copyfileobj(f_in, f_out)\n",
    "    print(f'Arquivo {nome_arquivo} descomprimido para {nome_arquivo[:-3]}')\n",
    "\n",
    "# Exemplo de uso\n",
    "descomprimir_arquivo('./dataset/Coffee Shop Sales.txt.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "def comprimir_arquivo_txt(arquivo_txt, arquivo_zip):\n",
    "    with zipfile.ZipFile(arquivo_zip, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "        zipf.write(arquivo_txt, arcname=arquivo_txt)\n",
    "\n",
    "# Exemplo de uso\n",
    "comprimir_arquivo_txt('./dataset/Coffee Shop Sales 2.txt', 'exemplo.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "def recuperar_arquivo_txt(arquivo_zip, arquivo_destino):\n",
    "    with zipfile.ZipFile(arquivo_zip, 'r') as zipf:\n",
    "        zipf.extractall()\n",
    "        # Extraindo apenas o arquivo txt\n",
    "        arquivo_extraido = zipf.namelist()[0]  # obtendo o nome do primeiro arquivo no zip\n",
    "        # Renomeando o arquivo extraído para o destino\n",
    "        import os\n",
    "        os.rename(arquivo_extraido, arquivo_destino)\n",
    "\n",
    "# Exemplo de uso\n",
    "recuperar_arquivo_txt('dataset/Coffee Shop Sales.zip', 'exemplo_recuperado.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo exemplo.txt comprimido em exemplo.tar.\n"
     ]
    }
   ],
   "source": [
    "import tarfile\n",
    "\n",
    "def compress_text_file(input_file, output_file):\n",
    "    with tarfile.open(output_file, \"w\") as tar:\n",
    "        tar.add(input_file, arcname=input_file)\n",
    "\n",
    "# Exemplo de uso\n",
    "input_txt_file = \"exemplo.txt\"  # Nome do seu arquivo .txt\n",
    "output_tar_file = \"exemplo.tar\"  # Nome do arquivo compactado\n",
    "compress_text_file(input_txt_file, output_tar_file)\n",
    "print(f\"Arquivo {input_txt_file} comprimido em {output_tar_file}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivos extraídos para ./test\n"
     ]
    }
   ],
   "source": [
    "import tarfile\n",
    "\n",
    "def decompress_tar_file(input_file, output_dir):\n",
    "    with tarfile.open(input_file, \"r\") as tar:\n",
    "        tar.extractall(path=output_dir)\n",
    "        print(f\"Arquivos extraídos para {output_dir}\")\n",
    "\n",
    "# Exemplo de uso\n",
    "input_tar_file = \"dataset/Coffee Shop Sales.tar\"  # Nome do arquivo .tar que você gerou\n",
    "output_directory = \"./test\"  # Diretório onde o arquivo .txt será extraído\n",
    "decompress_tar_file(input_tar_file, output_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_filepath = './test/dataset/Coffee Shop Sales.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "int,date,time,int,int,string,int,float,string,string,string\n",
      "\n",
      "149116\n",
      "\n",
      "4\n",
      "\n",
      "213019\n",
      "\n",
      "63905\n",
      "\n",
      "2024-09-22 12:55:07\n",
      "\n",
      "2024-09-22 12:55:07\n",
      "\n",
      "################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################\n",
      "\n",
      "################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################\n",
      "\n",
      "################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(txt_filepath, 'r') as file:\n",
    "    lines = file.readlines()\n",
    "    for i in range(0, 10):\n",
    "        print(lines[i+3])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "General_Work_Kernel",
   "language": "python",
   "name": "general_work_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
