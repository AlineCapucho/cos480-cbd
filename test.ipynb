{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel('./dataset/Coffee Shop Sales.xlsx')\n",
    "data.to_csv('./dataset/Coffee Shop Sales.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = pd.read_csv('./dataset/Coffee Shop Sales.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'transaction_id': 6, 'transaction_date': 10, 'transaction_time': 8, 'transaction_qty': 1, 'store_id': 1, 'store_location': 15, 'product_id': 2, 'unit_price': 5, 'product_category': 18, 'product_type': 21, 'product_detail': 28}\n",
      "[6, 10, 8, 1, 1, 15, 2, 5, 18, 21, 28]\n"
     ]
    }
   ],
   "source": [
    "columns = data2.columns\n",
    "size_per_column = dict()\n",
    "for column in columns:\n",
    "    elements = data2[column].unique()\n",
    "    elements_size = [len(str(elem)) for elem in elements]\n",
    "    size_per_column[column] = max(elements_size)\n",
    "print(size_per_column)\n",
    "print(list(size_per_column.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fixed_Size_Heap:\n",
    "    def __init__(self, block_size, field_sizes, filename):\n",
    "        self.block_size = block_size\n",
    "        self.field_sizes = field_sizes\n",
    "        self._set_record_size()\n",
    "        self.blocks = []\n",
    "        self._read_file(filename)\n",
    "        self.deleted_records = []\n",
    "\n",
    "    def _set_field_names(self, line):\n",
    "        self.field_names = line.split(',')\n",
    "\n",
    "    def _set_record_size(self):\n",
    "        self.record_size = sum(self.field_sizes) + len(self.field_sizes)\n",
    "    \n",
    "    def _padding(self, field, field_id):\n",
    "        diff = self.field_sizes[field_id] - len(field)\n",
    "        padded_field = field + (' ' * diff)\n",
    "        return padded_field\n",
    "\n",
    "    def _format_record(self, record):\n",
    "        formatted_record = ''\n",
    "        fields = record.strip().split(',')\n",
    "        for i in range(len(fields)):\n",
    "            if len(fields[i]) < self.field_sizes[i]:\n",
    "                padded_field = self._padding(fields[i], i)\n",
    "                formatted_record += padded_field + ','\n",
    "            else:\n",
    "                formatted_record += fields[i] + ','\n",
    "        return formatted_record\n",
    "\n",
    "    def _write_record(self, record):\n",
    "        if self.blocks == []:\n",
    "            self.blocks.append(self._format_record(record))\n",
    "        elif len(self.blocks[-1]) + self.record_size < self.block_size:\n",
    "            self.blocks[-1] += self._format_record(record)\n",
    "        else:\n",
    "            self.blocks.append(self._format_record(record))\n",
    "\n",
    "    def _read_file(self, filename):\n",
    "        with open(filename, 'r') as file:\n",
    "            self._set_field_names(file.readline())\n",
    "            for record in file:\n",
    "                self._write_record(record)\n",
    "    \n",
    "    def _search(self, field_id, value):\n",
    "        field_size = self.field_sizes[field_id]\n",
    "        number_of_records = math.floor(self.block_size / self.record_size)\n",
    "        success = False\n",
    "        for i in range(len(self.blocks)):\n",
    "            if success and field_id == 0:\n",
    "                break\n",
    "            for j in range(0, number_of_records):\n",
    "                offset = self.record_size * j\n",
    "                if self.field_sizes[:field_id] != []:\n",
    "                    offset += sum(self.field_sizes[:field_id]) + field_id\n",
    "                field_value = self.blocks[i][offset:offset + field_size].strip()\n",
    "                if field_value == '':\n",
    "                    continue\n",
    "                if field_value == value:\n",
    "                    yield [i, j]\n",
    "                    success = True\n",
    "                    if field_id == 0:\n",
    "                        break\n",
    "        return [-1, -1]\n",
    "\n",
    "    def _select(self, select_container, block_id, record_id):\n",
    "        offset = self.record_size * record_id\n",
    "        record = self.blocks[block_id][offset:offset + self.record_size]\n",
    "        select_container.append(record)\n",
    "\n",
    "    def select_by_single_primary_key(self, key):\n",
    "        select_container = []\n",
    "        for (i, j) in self._search(field_id=0, value=key):\n",
    "            if i == -1 and j == -1:\n",
    "                raise Exception('SelectionError: Primary Key nonexistent.')\n",
    "            else:\n",
    "                self._select(select_container=select_container, block_id=i, record_id=j)\n",
    "        return select_container\n",
    "    \n",
    "    def select_by_multiple_primary_key(self, keys):\n",
    "        select_container = []\n",
    "        exception_counter = 0\n",
    "        for key in keys:\n",
    "            for (i, j) in self._search(field_id=0, value=key):\n",
    "                if i == -1 and j == -1:\n",
    "                    exception_counter += 1\n",
    "                else:\n",
    "                    self._select(select_container=select_container, block_id=i, record_id=j)\n",
    "        if exception_counter == len(keys):\n",
    "            raise Exception('SelectionError: Primary Keys nonexistent.')\n",
    "        return select_container\n",
    "    \n",
    "    def select_by_field_interval(self, field, lower_limit, upper_limit):\n",
    "        field_id = self.field_names.index(field)\n",
    "        select_container = []\n",
    "        exception_counter = 0\n",
    "        for value in range(int(lower_limit), int(upper_limit)):\n",
    "            for (i, j) in self._search(field_id=field_id, value=str(value)):\n",
    "                if i == -1 and j == -1:\n",
    "                    exception_counter += 1\n",
    "                else:\n",
    "                    self._select(select_container=select_container, block_id=i, record_id=j)\n",
    "        if exception_counter == int(upper_limit) - int(lower_limit):\n",
    "            raise Exception('SelectionError: Requested Records nonexistent.')\n",
    "        return select_container\n",
    "\n",
    "    def _delete_record(self, block_id, record_id):\n",
    "        offset = self.record_size * record_id\n",
    "        head = self.blocks[block_id][:offset]\n",
    "        body = ' ' * self.record_size\n",
    "        tail = self.blocks[block_id][offset + self.record_size:]\n",
    "        self.blocks[block_id] = head + body + tail\n",
    "        self.deleted_records.append([block_id, record_id])\n",
    "\n",
    "    def delete_record_by_primary_key(self, key):\n",
    "        for (i, j) in self._search(field_id=0, value=key):\n",
    "            if i == -1 and j == -1:\n",
    "                raise Exception('DeleteError: Primary Key nonexistent.')\n",
    "            else:\n",
    "                self._delete_record(block_id=i, record_id=j)\n",
    "    \n",
    "    def delete_record_by_criterion(self, field, value):\n",
    "        field_id = self.field_names.index(field)\n",
    "        for (i, j) in self._search(field_id=field_id, value=value):\n",
    "            if i == -1 and j == -1:\n",
    "                raise Exception('DeleteError: Field Value nonexistent.')\n",
    "            else:\n",
    "                self._delete_record(block_id=i, record_id=j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = './dataset/Coffee Shop Sales.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "myfile = Fixed_Size_Heap(\n",
    "                    block_size=512,\n",
    "                    field_sizes=list(size_per_column.values()),\n",
    "                    filename=filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1     ,2023-01-01,07:06:11,2,5,Lower Manhattan,32,3.0  ,Coffee            ,Gourmet brewed coffee,Ethiopia Rg                 ,2     ,2023-01-01,07:08:56,2,5,Lower Manhattan,57,3.1  ,Tea               ,Brewed Chai tea      ,Spicy Eye Opener Chai Lg    ,3     ,2023-01-01,07:14:04,2,5,Lower Manhattan,59,4.5  ,Drinking Chocolate,Hot chocolate        ,Dark chocolate Lg           ,4     ,2023-01-01,07:20:24,1,5,Lower Manhattan,22,2.0  ,Coffee            ,Drip coffee          ,Our Old Time Diner Blend Sm ,'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myfile.blocks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myfile.delete_record_by_primary_key('3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'5     ,2023-01-01,07:22:41,2,5,Lower Manhattan,57,3.1  ,Tea               ,Brewed Chai tea      ,Spicy Eye Opener Chai Lg    ,6     ,2023-01-01,07:22:41,1,5,Lower Manhattan,77,3.0  ,Bakery            ,Scone                ,Oatmeal Scone               ,                                                                                                                                                                                                                                                            '"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myfile.blocks[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "myfile.delete_record_by_criterion(field='product_category', value='Coffee')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "myfile.delete_record_by_criterion(field='transaction_date', value='2023-01-01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['3     ,2023-01-01,07:14:04,2,5,Lower Manhattan,59,4.5  ,Drinking Chocolate,Hot chocolate        ,Dark chocolate Lg           ,']\n"
     ]
    }
   ],
   "source": [
    "select_container = myfile.select_by_single_primary_key('3')\n",
    "print(select_container)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['3     ,2023-01-01,07:14:04,2,5,Lower Manhattan,59,4.5  ,Drinking Chocolate,Hot chocolate        ,Dark chocolate Lg           ,', '5     ,2023-01-01,07:22:41,2,5,Lower Manhattan,57,3.1  ,Tea               ,Brewed Chai tea      ,Spicy Eye Opener Chai Lg    ,', '6     ,2023-01-01,07:22:41,1,5,Lower Manhattan,77,3.0  ,Bakery            ,Scone                ,Oatmeal Scone               ,']\n"
     ]
    }
   ],
   "source": [
    "select_container = myfile.select_by_multiple_primary_key(['3', '5', '6'])\n",
    "print(select_container)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "6\n",
      "7\n",
      "['1     ,2023-01-01,07:06:11,2,5,Lower Manhattan,32,3.0  ,Coffee            ,Gourmet brewed coffee,Ethiopia Rg                 ,', '2     ,2023-01-01,07:08:56,2,5,Lower Manhattan,57,3.1  ,Tea               ,Brewed Chai tea      ,Spicy Eye Opener Chai Lg    ,', '3     ,2023-01-01,07:14:04,2,5,Lower Manhattan,59,4.5  ,Drinking Chocolate,Hot chocolate        ,Dark chocolate Lg           ,', '4     ,2023-01-01,07:20:24,1,5,Lower Manhattan,22,2.0  ,Coffee            ,Drip coffee          ,Our Old Time Diner Blend Sm ,', '5     ,2023-01-01,07:22:41,2,5,Lower Manhattan,57,3.1  ,Tea               ,Brewed Chai tea      ,Spicy Eye Opener Chai Lg    ,']\n",
      "47782\n"
     ]
    }
   ],
   "source": [
    "select_container = myfile.select_by_field_interval(field='store_id', lower_limit='5', upper_limit='8')\n",
    "print(select_container[:5])\n",
    "print(len(select_container))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "General_Work_Kernel",
   "language": "python",
   "name": "general_work_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
